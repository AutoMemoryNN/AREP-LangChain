{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de LangChain LLM Chain\n",
    "\n",
    "En este tutorial aprender√°s a construir una aplicaci√≥n sencilla de LLM con LangChain utilizando modelos de chat y plantillas de prompts. La aplicaci√≥n traducir√° texto de ingl√©s a otro idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n inicial\n",
    "\n",
    "Primero, instalamos las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar las dependencias necesarias\n",
    "!pip install -qU \"langchain[openai]\" python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora configuramos nuestras variables de entorno y API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables desde .env si existe\n",
    "load_dotenv()\n",
    "\n",
    "# Si no est√° configurada la API key, solicitarla al usuario\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Introduce tu API key de OpenAI: \")\n",
    "\n",
    "# Configurar LangSmith para trazabilidad (opcional)\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\") and input(\"¬øDeseas configurar LangSmith para trazabilidad? (s/n): \").lower() == 's':\n",
    "    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Introduce tu API key de LangSmith: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utilizando Modelos de Lenguaje\n",
    "\n",
    "Primero, inicializamos un modelo de chat de OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usaremos el modelo directamente con una lista de mensajes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Spanish\"),\n",
    "    HumanMessage(\"Hello, how are you?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming de respuestas\n",
    "\n",
    "Los modelos de chat tambi√©n soportan streaming de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into French\"),\n",
    "    HumanMessage(\"Hello, how are you?\"),\n",
    "]\n",
    "\n",
    "print(\"Streaming tokens:\")\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilizando Plantillas de Prompts\n",
    "\n",
    "Las plantillas de prompts nos permiten estructurar nuestras entradas al modelo de forma m√°s flexible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos formatear este template con diferentes valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "print(\"Mensajes formateados:\")\n",
    "for message in prompt.to_messages():\n",
    "    print(f\"Rol: {message.type}\")\n",
    "    print(f\"Contenido: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, invocamos el modelo con el prompt formateado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(f\"Traducci√≥n a italiano: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aplicaci√≥n interactiva de traducci√≥n\n",
    "\n",
    "Vamos a crear una interfaz interactiva para nuestra aplicaci√≥n de traducci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text():\n",
    "    print(\"üåê Traductor de idiomas con LangChain üåê\")\n",
    "    print(\"----------------------------------\")\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nüìù Texto en ingl√©s (o 'salir' para terminar): \")\n",
    "        if text.lower() in [\"salir\", \"exit\", \"quit\"]:\n",
    "            print(\"üëã ¬°Hasta pronto!\")\n",
    "            break\n",
    "        \n",
    "        language = input(\"üåç Idioma de destino (e.g., Spanish, French, Italian): \")\n",
    "        \n",
    "        print(\"\\nüîÑ Traduciendo...\")\n",
    "        \n",
    "        # Formatear el prompt y obtener la traducci√≥n\n",
    "        prompt = prompt_template.invoke({\"language\": language, \"text\": text})\n",
    "        try:\n",
    "            response = model.invoke(prompt)\n",
    "            print(f\"\\n‚úÖ Traducci√≥n ({language}):\")\n",
    "            print(f\"üó£Ô∏è {response.content}\")\n",
    "            \n",
    "            # Mostrar informaci√≥n de tokens si est√° disponible\n",
    "            if hasattr(response, 'response_metadata') and 'token_usage' in response.response_metadata:\n",
    "                usage = response.response_metadata['token_usage']\n",
    "                print(f\"\\n‚ÑπÔ∏è Tokens utilizados: {usage.get('total_tokens', 'N/A')}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error durante la traducci√≥n: {str(e)}\")\n",
    "\n",
    "# Descomentar la siguiente l√≠nea para ejecutar la aplicaci√≥n interactiva\n",
    "# translate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "En este tutorial has aprendido:\n",
    "\n",
    "1. C√≥mo usar modelos de lenguaje de chat con LangChain\n",
    "2. C√≥mo implementar plantillas de prompts para estructurar tus entradas\n",
    "3. C√≥mo hacer streaming de respuestas del modelo\n",
    "4. C√≥mo construir una aplicaci√≥n interactiva de traducci√≥n\n",
    "\n",
    "Estos conceptos b√°sicos te servir√°n como fundamento para construir aplicaciones m√°s complejas utilizando LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}